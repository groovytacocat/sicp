#lang sicp
; Procedures just to prevent IDE from showing errors in file
(define (square x) (* x x))
(define (abs x) (if (< x 0) (- x) x))
(define (even? x) (= (remainder x 2) 0))
(define (double n) (* 2 n))
(define (halve n) (/ n 2))
(define (divides? a b)
  (= (remainder b a) 0))
(define (fermat-test n)
  (define (try-it a)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))
(define (fast-prime? n times)
  (cond [(= times 0) true]
        [(fermat-test n) (fast-prime? n (- times 1))]
        [else false]))
(define (sum term a next b)
  (if (> a b)
      0
      (+ (term a)
         (sum term (next a) next b))))
#|
Exercise 1.9
Each of the following two procedures defines a method for adding two positive integeres in terms of the preocedures inc, which increments its argument by 1,
and dec, which decrements its argument by 1

(define (+ a b)
  (if (= a 0)
      b
      (inc (+ (dec a) b))))

(define (+ a b)
  (if (= a 0)
      b
      (+ (dec a) (inc b))))

Using the substitution model, illustrate the process generated by each procedure in evaluating (+ 4 5).
Are these processes iterative or recursive
|#

#|
First:
(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5))))
(inc (inc (inc (inc (+ 0 5)))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9
This process is recursive

Second:
(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9
This process is iterative
|#

#|
Exercise 1.10
The following procedure computes a mathematical function called Ackermann's function

(define (A x y)
  (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1)
                 (A x (- y 1)))))))

What are the values of the following expressions?
(A 1 10)
(A 2 4)
(A 3 3)

Consider the following procedures, where A is the procedure defined above:
(define (f n) (A 0 n))
(define (g n) (A 1 n))
(define (h n) (A 2 n))
(define (k n) (* 5 n n))

Give concise mathematical definitions for the functions computed by the procedures f, g, and h for positive integer values of n. For example, (k n) computes 5n^2
|#

#|
(A 1 10) -> 1024
(A 2 4) -> 65536
(A 3 3) -> 65536

f(n) = 2n
g(n) = 2^n
h(n) = 2^h(n-1)
|#

#|
Exercise 1.11
A function f is defined by the rule that f(n) = n if n < 3 and f(n) = f(n-1) + 2f(n-2) + 3f(n-3) if n >= 3.
Write a procedure that coputes f by means of a recursive process. Write a procedure that computes f by means of an iterative process
|#

; Recursive
(define (f n)
  (if (< n 3)
      n
      (+ (f (- n 1))
         (* 2
            (f (- n 2)))
         (* 3
            (f (- n 3))))))

; Iterative
(define (f-iter n)
  (define (iter a b c count)
    (if (= count 0)
        c
        (iter (+ a
                 (* 2 b)
                 (* 3 c))
              a
              b
              (- count 1))))
  (iter 2 1 0 n))

#|
Exercise 1.12
Write a procedure that computes elements of Pascal's triangle by means of a recursive process
|#

(define (pascal n)
  (define (pascal-iter i j)
    (define (pascal-elem row col)
      (if (or (= col row) (= col 0))
          1
          (+ (pascal-elem (- row 1)
                          col)
             (pascal-elem (- row 1)
                          (- col 1)))))
    (cond [(<= i n)
           (cond [(<= j i)
                  (display (pascal-elem i j))
                  (display "\t")
                  (pascal-iter i (+ j 1))]
                 [(newline)
                  (pascal-iter (+ i 1) 0)])]))
  (pascal-iter 0 0))

#|
Exercise 1.15
The sine of an angle (in radians) can be copmuted by making use of the approximation sin(x) ~ x if x is sufficiently small, and the trig identity

sin(x) = 3 sin(x/3) - 4 sin^3 (x/3)

to reduce the size of the argument of sin. (For this exercise an angle is considered "sufficiently small" if its magnitude is not greater than 0.1 radians)
These ideas are incorporated into the following procedures
(define (cube x) (* x x x))
(define (p x) (- (* 3 x) (* 4 (cube x))))
(define (sine angle)
  (if (not (> (abs angle) 0.1))
      angle
      (p (sine (/ angle 3.0)))))

a. How many times is the procedure p applied when (sine 12.15) is evaluated?
b. What is the order of growth in space and number of steps (as a function of a) used by the process generated by the sine procedure when (sine a) is evaluated?

|#

#|
a. p is evaluated 5 times. My initial assumption was that p would be evaluated 3 times, since the angle is divided by 3 I assumed it would happen log_3(angle) number of times, however I was not taking into consideration the fact that it would need to perform the division by 3 until it was < 0.1
(sine 12.15)
(p (sine (4.05)))
(p (p (sine (1.35))))
(p (p (p (sine (0.45)))))
(p (p (p (p (sine (0.15))))))
(p (p (p (p (p (sine (0.05)))))))
|#

#|
b. This process's order of growth is Theta(log(a)) for both time and space. The space is due to being a recursive process so the stack grows proportional to the number of steps being executed.
The time complexity for this is log(a) as p is evaluated a number of times n that satisfies (a / 3^n ) < 0.1 -> n > log_3()
|#


#|
Exercise 1.16
Designa  procedure that evolves an iterative exponentiation process that uses successive squaring.
(Hint: Using the observation that (b^n/2)^2 = (b^2)^n/2, keep, along with the exponent n and the base b, and additional state variable a, and define the state transformation in such a way that the product a * b^n is unchanged from state to state.
      At the beginning of the process a is taken to be 1, and the answer is given by the value of a at the end of the process. In general, the technique of defining an invariant quantity that remains unchanged from state to state is a powerful
      way to think about the design of iterative algorithms)
|#

(define (fast-expt-iter b n)
  (define (exp-iter a b n)
    (cond [(= n 0) a]
          [(even? n) (exp-iter a (square b) (/ n 2))]
          [else (exp-iter (* a b) b (- n 1))]))
  (exp-iter 1 b n))

#|
Exercise 1.17
The Exponentiation algorithm in this section are based on performing exponentiation y means of repeated multiplication. In a similar way, one can perform integer multiplication by means of repeated addition.
The following multiplication procedure (in which it is assumed that our language can only add, not multiply) is analogous to the expt procedure

(define (* a b)
  (if (= b 0)
      0
      (+ a (* a (- b)))))

This algorithm takes a number of steps that is linear in b. Now suppose we include, together with addition, operations double, which doubles an integer, and halve which divides an (even) integer by 2.
Using thse design a multiplication procedure analogous to fast-expt that uses a logarithmic number of steps
|#

(define (fast-mult a b)
  (cond [(= b 0)
         0]
        [(even? b)
         (fast-mult (double a) (halve b))]
        [else
         (+ a (fast-mult a (- b 1)))]))

#|
Exercsise 1.18
Using the results of exercises 1.16 and 1.17, devise a procedure that generates an iterative process or multiplying 2 integers in terms of adding, doubling, and halving and uses a logarithmic number of steps
|#

(define (fast-mult-iter a b)
  (define (fast-iter a b prod)
    (cond [(= b 0)
           prod]
          [(even? b)
           (fast-iter (double a) (halve b) prod)]
          [else
           (fast-iter a (- b 1) (+ prod a))]))
  (fast-iter a b 0))

#|
Exercise 1.19
a = a + b, and b = a is called the transformation T. Observe that applying T repeatedly n times starting with 1 and 0, produces the pair Fib(n+1) and Fib(n).
In other words the Fibonacci numbers are produced by applying T^n, the nth power of transformation T starting with the pair (1, 0). Now consider T to be the special case of p = 0 and q = 1in a family of transformations T_pq where T_pq transforms (a,b) according to:
a = b*q + a*q + a*p and b = b*p + a*q.

Show that if we apply such a transformation T_pq twice, the effect is the same as using a single transformation T_p'q' of the same form, and compute p' and q' in terms of p and q
|#
(define (fib n)
  (fib-iter 1 0 0 1 n))

(define (fib-iter a b p q count)
  (cond [(= count 0)
         b]
        [(even? count)
         (fib-iter a
                   b
                   (+ (square p) (square q))
                   (+ (square q) (* 2 p q))
                   (/ count 2))]
        [else (fib-iter (+ (* b q) (* a q) (* a p))
                        (+ (* b p) (* a q))
                        p
                        q
                        (- count 1))]))

#|
Exercise 1.20
Process generated by procedures depends on interpreter rules. Suppose we were to itnerpret this procedure using normal-order evaluation.
Using the subsitution method (for normal order), illustrate the process generated in evaluating (gcd 206 40) and indicate the remainder operations that are actually performed.

How many remainder operations are actually performed in the normal-order evaluation of (gc 206 40)? In the applicative order evaluation?
|#

; Applicative Order - remainder is evaluated 4 times.
; Normal Order evaluation - remainder is evalauted 18 times. (too much paper space wasted doing all this and I want to type it out even less)

#|
Exercise 1.21
Use the smallest-divisor procedurfe to find the smallest divisor of each of the following numbers: 199, 1999, 19999.
|#
;(smallest-divisor 199)    199
;(smallest-divisor 1999)   1999
;(smallest-divisor 19999)  7

#|
Exercise 1.22
Most Lisp implementations include a primitive called runtime that returns an integer that specifies the amount of time the system has been running (measured for example, in microseconds).
The following timed-prime-test procedure, when called with an integer n, prints n and checks to see if n is prime. If n is prime, the procedure prints three asterisks followed by the amount of time used in performing the test

(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))

(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime) start-time))))

(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time))

Using this procedure, write a procedure search-for-primes that checks the primality of consecutive odd integers in a specified range.
Use your procedure to find:
  The three smallest primes larger than 1000
  Larger than 10,000
  Larger than 100,000
  Larger than 1,000,000

Note the time needed to test each prime. Since testing for primes around 10,000 should take about sqrt(10) times as long as testing for primes around 1000.
Do your timing data bear this out? How well do the data for 100,000 and 1,000,000 support the sqrt(n) prediction?
Is your result compatible with the notion that programs on your machine run in time proportional to the number of steps required for the computation?
|#

; Larger than 1,000
; 1009 | 1013 | 1019

; Larger than 10,000
; 10007 | 10009 | 10037

; Larger than 100,000
; 100003 | 100019 | 100043

; Larger than 1,000,000
; 1000003 | 1000033 | 1000037

; Timing stats for testing did not seem to match up at all with sqrt(n) prediction, and I attribute this to the use of

#|
Exercise 1.23
The smallest-divisor procedure shown at the start of this section does lots of needless testing:
After it checks to see if the number is divisible by 2 there is no point in checking to see if it is divisible by any larger even numbers.
This suggests that the values used for test-divisor should not be 2, 3, 4, 5, 6... but rather 2, 3, 5, 7, 9...
To implement this change define a procedure named next that returns 3 if its input is equal to 2 and otherwise returns its input plus 2.
Modify the smallest-divisor procedure to use (next test-divisor) instead of (+ test-divisor 1). With timed-prime-test incorporating this modified version of smallest-divisor,
run the test for each of the 12 primes found in exercise 1.22. Since this modification halves the number of test steps, you should expect it to run about twice as fast.
Is this expectation confirmed? If not, what is the observed ratio of the speeds of the two algorithms, and how do you explain the fact that it is different from 2?
|#

(define (smallest-divisor n)
  (define (find-divisor n test-divisor)
    (define (next num)
      (if (= num 2)
          3
          (+ num 2)))
    (cond [(> (square test-divisor) n) n]
          [(divides? test-divisor n) test-divisor]
          [else (find-divisor n (next test-divisor))]))
  (find-divisor n 2))

; Speed up did not appear to be twice as fast. I assume this is potentially just a result of modern computing speeds
; Additionally I assume that it comes from calling the next procedure now which is not a primitive as well as evaluating the if statement in the next procedure

#|
Exercise 1.24
Modify the timed-prime-test procedure of exercise 1.22 to use fast-prime?, and test each of the 12 primes you found in that exercise.
Since the Fermat test has Theta(log(n)) growth, how would you expect the time to test primes near 1,000,000 to compare with the time needed to test primes near 1000?
Do your data bear this out? Can you explain any discrepancy you can find?
|#

(define (timed-prime-test n)
  (define (start-prime-test n start-time)
    (define (report-prime elapsed-time)
      (display " *** ")
      (display elapsed-time))
    (if (fast-prime? n 2)
        (report-prime (- (runtime) start-time))))
  (newline)
  (display n)
  (start-prime-test n (runtime)))


#|
Exercise 1.25
Alyssa P. Hacker complains that we went to a lot of extra work in writing expmod. After all, she says, since we already know how to compute exponentials,
we could have simply written

(define (expmod base exp m)
  (remainder (fast-expt base exp) m))

Is she correct? Would this procedure serve as well for our fast prime tester? Explain?
|#

; No this procedure takes significantly longer to evaluate, as it will try and calculate the exponentiation using successive squaring without taking the remainder between each
; squaring as the author's does

#|
Exercise 1.26
Louis Reasoner is having great difficulty doing exercise 1.24. His fast-prime? test seems to run more slowly than his prime? test. Louis calls his friend Eva Lu Ator over to help.
When they examine Louis's code, they find that he has rewritten the expmod procedure to use an explicit multiplication, rather than calling square:

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
          (remainder (* (expmod base (/ exp 2) m)
                        (expmod base (/ exp 2) m))
                        m))
        (else
         (remainder (* base (expmod base (- exp 1) m))
                    m))))

"I don't see what difference that could make," says Louis. "I do." says Eva. "By writing the procedure like that, you have transformed the Theta(log(n)) process into a Theta(n) process"
Explain
|#

; This is because instead of evaluating (expmod base (/ exp 2) m) and then squaring it, the procedure is making 2 duplicate recursive calls. This squares the number of steps to execute
; and cancels out the logarithmic step reduction of halving the base leading to Theta(n)

#|
Exercise 1.27
Demonstrate that the Carmichael numbers listed in footnote 47 really do fool the Fermat test.
That is, write a procedure that takes an integer n and tests whether a^n is congruent to a modulo n for every a < n try your procedure on the given Carmichael numbers
(561, 1105, 1729, 2465, 2821, 6601)
|#

#|
Code:
(define (carmichael n)
  (define (car-iter cur)
    (cond [(= cur n) true]
          [(= (expmod cur n n) cur) (car-iter (+ cur 1))]
          [else false]))
  (car-iter 1))

Results:
(carmichael 561) -> #t
(carmichael 1105) -> #t
(carmichael 1729) -> #t
(carmichael 2465) -> #t
(carmichael 2821) -> #t
(carmichael 6601) -> #t
|#

#|
Exercise 1.28
One variant of the Fermat test that cannot be fooled is called the Miller-Rabin test. This starts from an alternate form of Fermat's Little Theorem, which states that if n is a prime number
and a is any positive integer less than n, then a raised to the (n - 1)st power is congruent to 1 modulo n. To test the primality of a number n by the Miller-Rabin test, we pick a random
number a < n and raise a to the (n - 1)st power modulo n using the expmod procedure. However, whenever we perform the squaring step in expmod, we check to see if we have discovered a
"nontrivial square root of 1 modulo n", that is, a number not equal to 1 or n - 1 whose square is equal to 1 modulo n. It is possible to prove that if such a nontrivial square root of 1 exists, then n is not prime.
It is also possible to prove that if n is an odd number that is not prime, then, for at least half the numbers a < n, computing a^(n-1) in this way will reveal a nontrivial square root of 1 modulo n.

Modify the expmod procedure to signal if it discovers a nontrivial square root of 1, and use this test to implement the Miller-Rabin test with a procedure analogous to fermat-test.
(Hint: One convenient way to make expmod signal is to have it return 0)
|#

; Had to lookup help online for this, instead of doing (remainder (square base) m) I was doing (expmod base 2 m) which would loop infinitely
(define (expmod base exp m)
  (cond [(= exp 0)
         1]
        [(and (not (or (= base 1)
                       (= base (- m 1))))
              (= (remainder (square base) m) 1))
         0]
        [(even? exp)
         (remainder (square (expmod base (/ exp 2) m))
                    m)]
        [else
         (remainder (* base (expmod base (- exp 1) m))
                    m)]))

(define (miller-rabin n)
  (define (try-it a)
    (= (expmod a (- n 1) n) 1))
  (try-it (+ 1 (random (- n 1)))))


